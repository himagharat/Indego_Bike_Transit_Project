{\rtf1\ansi\ansicpg1252\cocoartf2577
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Day 1 - \
\
Created a repository, folders to hold csv files named resources, and  we created a documentation folder to hold this journal of steps taken throughout the process. Lasltly, we created an output folder to hold our smaller csv files. \
\
Uploaded 4 quarters of csv data into pandas and narrowed down to columns we wanted to look at.  Ran into an issue merging 2019 Q2 with 2019 Q3 data frames.  \
\
Issue: Because of the data frame sizes was unable to merge together. Steven\'92s kernel was timing out. The combined data frame would be around 3.2 million records.  There is a way to fix with python library Dask.  Do we want to go through the work of importing Dask and using the new library? If not we can export the files and get them uploaded into PostgreSQL. If we do have to download Dask.  Dask is supposed to come with an anaconda environment.  Steven ran into issues because Dask looks to be supported by earlier versions of python ,not 3.7 that we set up for class environment.  Dask is a very powerful tool that would allow us to combine the data frames and is essentially pandas on steroids.   \
\
Dask tutorial: https://docs.dask.org/en/latest/dataframe.html\
\
Day 1 continued: \
\
Entity relationship diagram (ERD) completed through (link: https://app.quickdatabasediagrams.com/#/d/wrDaKy) \
\
Thought process in creating the ERD was to be able to query Q2 and Q3 of each year as well as being able to do 2019 Q2 comparison to 2020 Q2, same with Q3.  We can change the primary key and foreign key arrangements as necessary.  \
\
\
Text to create the diagrams below for above ERD link:\
\
Q219\
-\
trip_id int PK\
duration int\
start_station string\
end_station string\
plan_duration int\
trip_route_category string\
passholder_type string\
bike_type string \
\
Q319\
-\
trip_id int PK FK >- Q219.trip_id\
duration int \
start_station string\
end_station string\
plan_duration int\
trip_route_category string\
passholder_type string\
bike_type string\
\
Q220\
-\
trip_id int PK FK >- Q219.trip_id\
duration int\
start_station string\
end_station string\
plan_duration int\
trip_route_category string\
passholder_type string\
bike_type string\
\
Q320\
-\
trip_id int FK >- Q220.trip_id FK >- Q319.trip_id\
duration int\
start_station string\
end_station string\
plan_duration int\
trip_route_category string\
passholder_type string\
bike_type string\
\
Steven:  I do not think it is necessary to merge, but I think consulting the TAs is worth while.  We will want to check if data frames and querying data through flask may be too big a task for local machines.  We can do more calculations too narrow down current data frames to summary tables once we decide which calculations we wish to make if problematic. \
\
Day 2:\
LESLIE FILL IN FOR WHAT WAS DONE IN CREATING PYMONGO.\
\
\
Day 3: Got PyMongo running on everyones local machines and began to work out app.py for flask PyMongo.  Used MongoDB compass and local host to connect Jupyter Notebook file to MongoDB Compass. On Mac you do have to create instance to start MongoDB.  Can be done with Mongo - - port <port name>. \
\
Conversation with David.  We have to set up the app.py similar to the SQLAlchemy HW. It is best to create routes for all of the information we are looking to query. Start with duration, then bike type, pass holder count, etc. David recommended that it is best to query and do the bulk of calculations in python because it would be quicker.  JavaScript is not a quick with handling large amounts of information.  He also noted that it was best to think of the data lifecycle and to start with app.py sending data to an index.html file and then to .js file and then back to index.html.  He also mentioned to start small and then grow, so query 100 rows of information first and then move to creating variables and passing variables into .js file. \
\
\
Day 4 Saturday: Re-worked the Jupyter notebook for sending data to MongoDB.  Ended up making a lot of the calculations in Jupyter Notebook.  It was decided it would make it easier to query in the flask app.  We then worked on the flask app.  The flask app took some time because python did not want to return the json files.  Ultimately, had to manually parse the code with a for loop rather than return jsonify(collection).\
\
We cleaned up the Jupyter notebook files and deleted unnecessary documents in our repository since we pivoted.  We are now able to move on to our next set of tasks.  Creating charts with JavaScript and index.html file. }